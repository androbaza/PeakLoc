{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from localization_scripts.imports import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/smlm-workstation/event-smlm/our_ev_smlm_recordings/MT_5May_S2_reduced_bias_580sec/MT_5May_S2_reduced_bias_580sec.raw'\n",
    "if os.path.basename(filename)[-4:] == \".raw\":\n",
    "    events = raw_events_to_array(filename).astype(\n",
    "        [(\"x\", \"uint16\"), (\"y\", \"uint16\"), (\"p\", \"byte\"), (\"t\", \"uint64\")]\n",
    "    )\n",
    "\n",
    "slice = events[(events[\"t\"] < 50e6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the data using 24 cores... Events go brrrrrrrrrrrr!\n",
      "Converting events to dictionaries... Elapsed time: 0.10 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from localization_scripts.event_array_processing import array_to_polarity_map\n",
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "\"\"\"PROMINENCE is the prominence of the peaks in the convolved signals.\n",
    "Smaller value detects more peaks, increasing the evaluation time.\"\"\"\n",
    "PROMINENCE = 25\n",
    "\n",
    "\"\"\"DATASEET_FWHM is the FWHM of the PSF in the dataset in pixels.\"\"\"\n",
    "DATASEET_FWHM = 6\n",
    "\n",
    "\"\"\"PEAK_TIME_THRESHOLD is the maximum time difference between two peaks in order to be considered as the same peak.\"\"\"\n",
    "PEAK_TIME_THRESHOLD = 40e3\n",
    "\n",
    "\"\"\"PEAK_NEIGHBORS is the number of neighboring pixels to be considered when filtering same peaks.\"\"\"\n",
    "PEAK_NEIGHBORS = 7\n",
    "\n",
    "\"\"\"ROI_RADIUS is the radius of the generated ROI in pixels.\"\"\"\n",
    "ROI_RADIUS = 6\n",
    "\n",
    "events = slice\n",
    "start_time = time.time()\n",
    "\n",
    "# Get the minimum and maximum x and y coordinates\n",
    "min_x = events[\"x\"].min()\n",
    "min_y = events[\"y\"].min()\n",
    "max_x = events[\"x\"].max()\n",
    "max_y = events[\"y\"].max()\n",
    "\n",
    "# Create coordinate lists\n",
    "y_coords, x_coords = [min_y, max_y], [min_x, max_x]\n",
    "coords = generate_coord_lists(y_coords[0], y_coords[1], x_coords[0], x_coords[1])\n",
    "\n",
    "# Generate dictionaries and calculate max length\n",
    "print(f\"Analyzing the data using {NUM_CORES} cores... Events go brrrrrrrrrrrr!\")\n",
    "print(\n",
    "    f\"Converting events to dictionaries... Elapsed time: {time.time() - start_time:.2f} seconds\"\n",
    ")\n",
    "dict_events, events_t_p_dict, max_len = array_to_polarity_map(events, coords)\n",
    "# events_t_p_dict = array_to_time_map(events)\n",
    "# del events\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create signals, cleanup and slice data\n",
    "print(\n",
    "    f\"Creating convolved signals... Elapsed time: {time.time() - start_time:.2f} seconds\"\n",
    ")\n",
    "max_len = int(max_len * 2)\n",
    "times, cumsum, coordinates = create_convolved_signals(\n",
    "    dict_events, coords, max_len, NUM_CORES\n",
    ")\n",
    "\n",
    "# del dict_events, max_len\n",
    "\n",
    "print(f\"Finding peaks... Elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "peak_list = find_peaks_parallel(\n",
    "    times,\n",
    "    cumsum,\n",
    "    coordinates,\n",
    "    NUM_CORES,\n",
    "    prominence=PROMINENCE,\n",
    "    interpolation_coefficient=5,\n",
    "    spline_smooth=0.7,\n",
    ")\n",
    "peaks, prominences, on_times, coordinates_peaks = create_peak_lists(peak_list)\n",
    "peaks_dict = group_timestamps_by_coordinate(\n",
    "    coordinates_peaks, peaks, prominences, on_times\n",
    ")\n",
    "\n",
    "# possible to speed up with numba\n",
    "print(f\"Filtering peaks... Elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "unique_peaks = find_local_max_peak(\n",
    "    peaks_dict, threshold=PEAK_TIME_THRESHOLD, neighbors=PEAK_NEIGHBORS\n",
    ")\n",
    "\n",
    "out_folder_localizations = filename[:-4] + \"/\"\n",
    "temp_files_localization = out_folder_localizations + \"temp_files/\"\n",
    "if not os.path.exists(out_folder_localizations):\n",
    "    os.makedirs(out_folder_localizations)\n",
    "if not os.path.exists(temp_files_localization):\n",
    "    os.makedirs(temp_files_localization)\n",
    "\n",
    "# save_dict(\n",
    "#     unique_peaks,\n",
    "#     temp_files_localization\n",
    "#     + \"unique_peaks_fwhm_\"\n",
    "#     + str(DATASEET_FWHM)\n",
    "#     + \"_prominence_\"\n",
    "#     + str(PROMINENCE)\n",
    "#     + \"_time_slice_\"\n",
    "#     + str(time_slice)\n",
    "#     + \".pkl\",\n",
    "# )\n",
    "\n",
    "print(f\"Generating ROIs... Elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "rois = generate_rois(\n",
    "    unique_peaks,\n",
    "    events_t_p_dict,\n",
    "    roi_rad=ROI_RADIUS,\n",
    "    min_x=min_x,\n",
    "    min_y=min_y,\n",
    "    num_cores=NUM_CORES,\n",
    "    max_x=max_x,\n",
    "    max_y=max_y,\n",
    ")\n",
    "\n",
    "# print(\n",
    "#     f\"Performing localization... Elapsed time: {time.time() - start_time:.2f} seconds\"\n",
    "# )\n",
    "# localizations = perfrom_localization_parallel(rois, dataset_FWHM=DATASEET_FWHM)\n",
    "\n",
    "# print(f\"Finished! Total elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "# np.save(\n",
    "#     temp_files_localization\n",
    "#     + \"localizations_prominence_fwhm_\"\n",
    "#     + str(DATASEET_FWHM)\n",
    "#     + \"_prominence_\"\n",
    "#     + str(PROMINENCE)\n",
    "#     + \"_time_slice_\"\n",
    "#     + str(time_slice)\n",
    "#     + \".npy\",\n",
    "#     localizations,\n",
    "# )\n",
    "np.save(\n",
    "    temp_files_localization\n",
    "    + \"rois_prominence_fwhm_\"\n",
    "    + str(DATASEET_FWHM)\n",
    "    + \"_prominence_\"\n",
    "    + str(PROMINENCE)\n",
    "    + \".npy\",\n",
    "    rois,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed  0  % --> ~ 0  localizations found\n"
     ]
    }
   ],
   "source": [
    "rois = generate_rois(\n",
    "    unique_peaks,\n",
    "    events_t_p_dict,\n",
    "    roi_rad=ROI_RADIUS,\n",
    "    min_x=min_x,\n",
    "    min_y=min_y,\n",
    "    num_cores=NUM_CORES,\n",
    "    max_x=max_x,\n",
    "    max_y=max_y,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smlm-workstation/miniconda3/envs/mvis/lib/python3.10/site-packages/numba/core/decorators.py:262: NumbaDeprecationWarning: \u001b[1mnumba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.\u001b[0m\n",
      "  warnings.warn(msg, NumbaDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from localization_scripts.imports import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '/home/smlm-workstation/event-smlm/our_ev_smlm_recordings/MT_CL_2023-05-31_14-55-59.raw'\n",
    "filename = '/home/smlm-workstation/event-smlm/our_ev_smlm_recordings/MT_CL_06_08/recording_2023-06-08_16-18-59.raw'\n",
    "if os.path.basename(filename)[-4:] == \".raw\":\n",
    "    events = raw_events_to_array(filename).astype(\n",
    "        [(\"x\", \"uint16\"), (\"y\", \"uint16\"), (\"p\", \"byte\"), (\"t\", \"uint64\")]\n",
    "    )\n",
    "\n",
    "slice = events[(events[\"t\"] < 200e6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the data using 24 cores... Events go brrrrrrrrrrrr!\n",
      "Converting events to dictionaries... Elapsed time: 0.49 seconds\n"
     ]
    }
   ],
   "source": [
    "from numba import njit, prange, types, typed, jit\n",
    "from numba.typed import List\n",
    "# from localization_scripts.event_array_processing import array_to_polarity_map\n",
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "\"\"\"PROMINENCE is the prominence of the peaks in the convolved signals.\n",
    "Smaller value detects more peaks, increasing the evaluation time.\"\"\"\n",
    "PROMINENCE = 18\n",
    "\n",
    "\"\"\"DATASEET_FWHM is the FWHM of the PSF in the dataset in pixels.\"\"\"\n",
    "DATASEET_FWHM = 6\n",
    "\n",
    "\"\"\"PEAK_TIME_THRESHOLD is the maximum time difference between two peaks in order to be considered as the same peak.\"\"\"\n",
    "PEAK_TIME_THRESHOLD = 40e3\n",
    "\n",
    "\"\"\"PEAK_NEIGHBORS is the number of neighboring pixels to be considered when filtering same peaks.\"\"\"\n",
    "PEAK_NEIGHBORS = 7\n",
    "\n",
    "\"\"\"ROI_RADIUS is the radius of the generated ROI in pixels.\"\"\"\n",
    "ROI_RADIUS = 8\n",
    "\n",
    "events = slice\n",
    "start_time = time.time()\n",
    "\n",
    "# Get the minimum and maximum x and y coordinates\n",
    "min_x = events[\"x\"].min()\n",
    "min_y = events[\"y\"].min()\n",
    "max_x = events[\"x\"].max()\n",
    "max_y = events[\"y\"].max()\n",
    "\n",
    "# Create coordinate lists\n",
    "y_coords, x_coords = [min_y, max_y], [min_x, max_x]\n",
    "coords = generate_coord_lists(y_coords[0], y_coords[1], x_coords[0], x_coords[1])\n",
    "\n",
    "# Generate dictionaries and calculate max length\n",
    "print(f\"Analyzing the data using {NUM_CORES} cores... Events go brrrrrrrrrrrr!\")\n",
    "print(\n",
    "    f\"Converting events to dictionaries... Elapsed time: {time.time() - start_time:.2f} seconds\"\n",
    ")\n",
    "\n",
    "dict_events, events_t_p_dict, coords, max_length = convert_to_hashmaps(slice, filename, max_x, max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250473"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(dict_events.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250473"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "894145"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [(val[0] ,val[1][2][0]) for val in list(dict_events.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(719, 1037), 6094482],\n",
       "       [(719, 1038), 6185263],\n",
       "       [(719, 1039), 6976357],\n",
       "       [(719, 1040), 8067932],\n",
       "       [(719, 1041), 8476864],\n",
       "       [(719, 1042), 8874138],\n",
       "       [(719, 1043), 8956025],\n",
       "       [(719, 1044), 10119824],\n",
       "       [(719, 1045), 11574069],\n",
       "       [(719, 1046), 14708398]], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = np.sort(np.asarray(lengths), axis=0)\n",
    "lengths[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange, types, typed, jit\n",
    "from numba.typed import List\n",
    "\n",
    "@njit(cache=True, nogil=True, fastmath=True)\n",
    "def remove_coordinates(arr, my_dict, time_map):\n",
    "    for y in range(arr.shape[0]):\n",
    "        for x in range(arr.shape[1]):\n",
    "            if arr[y, x] == 0:\n",
    "                coord = (y, x)\n",
    "                if coord in my_dict:\n",
    "                    del my_dict[coord]\n",
    "                if coord in time_map:\n",
    "                    del time_map[coord]\n",
    "    return my_dict, time_map\n",
    "\n",
    "@njit(cache=True, nogil=True, fastmath=True)\n",
    "def remove_coordinates_by_list(arr, my_dict, time_map):\n",
    "    for coord in arr:\n",
    "        (y, x) = coord\n",
    "        if (y, x) in my_dict:\n",
    "            del my_dict[(y, x)]\n",
    "        if (y, x) in time_map:\n",
    "            del time_map[(y, x)]\n",
    "    return my_dict, time_map\n",
    "\n",
    "@njit(cache=True, nogil=True, fastmath=True)\n",
    "def fill_widefield(dict_events, max_x, max_y):\n",
    "    widefield = np.zeros((max_y+1, max_x+1), dtype=np.uint64)\n",
    "    for key in dict_events.keys():\n",
    "        widefield[key[0], key[1]] = dict_events[key][2][0]\n",
    "    return widefield\n",
    "\n",
    "@njit(nogil=True, cache=True, fastmath=True)\n",
    "def detect_outlier(data):\n",
    "    q1, q3 = np.percentile(data, [40, 99.99])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1\n",
    "    upper_bound = q3 + (4 * iqr)\n",
    "    outliers_id = []\n",
    "    for id, x  in enumerate(data):\n",
    "        if x <= lower_bound or x >= upper_bound:\n",
    "            outliers_id.append(id)\n",
    "    return outliers_id\n",
    "\n",
    "@njit(cache=True, nogil=True, fastmath=True)\n",
    "def array_to_polarity_map(arr):\n",
    "    \"\"\"\n",
    "    Converts a structured NumPy ndarray with fields x, y, p, t into a dictionary with keys as (x, y) pairs and\n",
    "    values as a nested dictionary with keys from p and corresponding values from t as a list for that coordinate pair.\n",
    "    \"\"\"\n",
    "    dict_out = {}\n",
    "    time_map = {}\n",
    "    # max_len = 0\n",
    "    for id in prange(len(arr)):\n",
    "        key = (arr[id][\"y\"], arr[id][\"x\"])\n",
    "        if key in dict_out:\n",
    "            dict_out[key][arr[id][\"p\"]].append(arr[id][\"t\"])\n",
    "        else:\n",
    "            dict_out[key] = {\n",
    "                0: List.empty_list(types.uint64), # negative\n",
    "                1: List.empty_list(types.uint64), # positive\n",
    "                2: List.empty_list(types.uint64), # sum of negative and positive num \n",
    "                3: List.empty_list(types.uint64), # negative num\n",
    "                4: List.empty_list(types.uint64)  # positive num\n",
    "            }\n",
    "            dict_out[key][arr[id][\"p\"]].append(arr[id][\"t\"])\n",
    "        if key in time_map:\n",
    "            time_map[key][arr[id][\"t\"]] = arr[id][\"p\"]\n",
    "        else:\n",
    "            time_map[key] = {arr[id][\"t\"]: arr[id][\"p\"]}\n",
    "        # if len(dict_out[key][1]) > max_len:\n",
    "        #     max_len = len(dict_out[key][1])\n",
    "        # if len(dict_out[key][0]) > max_len:\n",
    "        #     max_len = len(dict_out[key][0])\n",
    "    for key in dict_out.keys():\n",
    "        dict_out[key][2].append(len(dict_out[key][0]) + len(dict_out[key][1]))\n",
    "        dict_out[key][3].append(len(dict_out[key][0]))\n",
    "        dict_out[key][4].append(len(dict_out[key][1]))\n",
    "    return dict_out, time_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=10.5\n",
    "radius=11\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dict_events, time_map = array_to_polarity_map(events)\n",
    "widefield = fill_widefield(dict_events, max_x, max_y)\n",
    "plt.imsave(filename[:-4]+\"_widefield.png\", dpi=300, arr=widefield, cmap=\"gray\", vmax=widefield.mean())\n",
    "widefield_filtered = gaussian_filter(widefield, sigma=sigma, radius=radius)\n",
    "useful_pixels = np.where(widefield_filtered >= np.percentile(widefield_filtered, 55), widefield, 0)\n",
    "plt.imsave(filename[:-4]+\"_useful_pixels.png\", dpi=300, arr=useful_pixels, cmap=\"gray\", vmax=useful_pixels.mean()*3)\n",
    "dict_events, time_map = remove_coordinates(useful_pixels, dict_events, time_map)\n",
    "lengths = np.asarray([np.array((val[0] ,val[1][2][0]), dtype=[(\"c\", np.uint16, (2)), (\"l\", np.uint64)]) for val in list(dict_events.items())])\n",
    "lengths = np.sort(lengths, order=\"l\")\n",
    "indices = detect_outlier(lengths['l'])\n",
    "to_delete = lengths[indices]['c']\n",
    "filtered = np.delete(lengths, indices, axis=0)\n",
    "max_length = filtered[-1]['l']\n",
    "dict_events, time_map = remove_coordinates_by_list(to_delete, dict_events, time_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.asarray([np.array((val[0] ,val[1][2][0]), dtype=[(\"c\", np.uint16, (2)), (\"l\", np.uint64)]) for val in list(dict_events.items())])\n",
    "lengths = np.sort(lengths, order=\"l\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop\n"
     ]
    }
   ],
   "source": [
    "times, cumsum, coordinates = create_convolved_signals(\n",
    "    dict_events, coords, max_len=max_length*8, num_cores=NUM_CORES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Finding peaks... Elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "peak_list = find_peaks_parallel(\n",
    "    times,\n",
    "    cumsum,\n",
    "    coordinates,\n",
    "    NUM_CORES,\n",
    "    prominence=PROMINENCE,\n",
    "    interpolation_coefficient=5,\n",
    "    spline_smooth=0.7,\n",
    ")\n",
    "peaks, prominences, on_times, coordinates_peaks = create_peak_lists(peak_list)\n",
    "peaks_dict = group_timestamps_by_coordinate(\n",
    "    coordinates_peaks, peaks, prominences, on_times\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_peaks = find_local_max_peak(\n",
    "    peaks_dict, threshold=PEAK_TIME_THRESHOLD, neighbors=PEAK_NEIGHBORS\n",
    ")\n",
    "\n",
    "out_folder_localizations = filename[:-4] + \"/\"\n",
    "temp_files_localization = out_folder_localizations + \"temp_files/\"\n",
    "if not os.path.exists(out_folder_localizations):\n",
    "    os.makedirs(out_folder_localizations)\n",
    "if not os.path.exists(temp_files_localization):\n",
    "    os.makedirs(temp_files_localization)\n",
    "\n",
    "save_dict(\n",
    "    unique_peaks,\n",
    "    temp_files_localization\n",
    "    + \"unique_peaks_fwhm_\"\n",
    "    + str(DATASEET_FWHM)\n",
    "    + \"_prominence_\"\n",
    "    + str(PROMINENCE)\n",
    "    + \".pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating ROIs... Elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "rois = generate_rois(\n",
    "    unique_peaks,\n",
    "    events_t_p_dict,\n",
    "    roi_rad=ROI_RADIUS,\n",
    "    min_x=min_x,\n",
    "    min_y=min_y,\n",
    "    num_cores=NUM_CORES,\n",
    "    max_x=max_x,\n",
    "    max_y=max_y,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\n",
    "    f\"Performing localization... Elapsed time: {time.time() - start_time:.2f} seconds\"\n",
    ")\n",
    "localizations = perfrom_localization_parallel(rois, dataset_FWHM=DATASEET_FWHM)\n",
    "\n",
    "print(f\"Finished! Total elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "np.save(\n",
    "    temp_files_localization\n",
    "    + \"localizations_prominence_fwhm_\"\n",
    "    + str(DATASEET_FWHM)\n",
    "    + \"_prominence_\"\n",
    "    + str(PROMINENCE)\n",
    "    + \".npy\",\n",
    "    localizations,\n",
    ")\n",
    "np.save(\n",
    "    temp_files_localization\n",
    "    + \"rois_prominence_fwhm_\"\n",
    "    + str(DATASEET_FWHM)\n",
    "    + \"_prominence_\"\n",
    "    + str(PROMINENCE)\n",
    "    + \".npy\",\n",
    "    rois,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.axis(\"off\")\n",
    "widefield_filtered = gaussian_filter(widefield, sigma=9.5, radius=9)\n",
    "useful_pixels = np.where(widefield_filtered >= np.percentile(widefield_filtered, 60), widefield, 0)\n",
    "\n",
    "# plt.imshow(gaussian_filter(widefield), sigma=0.8, radius=5), cmap=\"gray\", vmax=30)\n",
    "plt.imshow(np.where(widefield_filtered >= np.percentile(widefield_filtered, 50), widefield, 0), cmap=\"gray\", vmax=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
